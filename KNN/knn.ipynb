{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "pd.set_option('display.max_columns',30)\n",
    "\n",
    "# Load data\n",
    "dataset_df1 = pd.read_csv(r\"./premonsoon/ground_water_quality_2018_pre.csv\")\n",
    "dataset_df2 = pd.read_csv(r\"./premonsoon/ground_water_quality_2019_pre.csv\")\n",
    "dataset_df3 = pd.read_csv(r\"./premonsoon/ground_water_quality_2020_pre.csv\")\n",
    "dataset_df4 = pd.read_csv(r\"./premonsoon/ground_water_quality_2021_pre.csv\")\n",
    "dataset_df5 = pd.read_csv(r\"./premonsoon/ground_water_quality_2022_pre.csv\")\n",
    "dataset_df6 = pd.read_csv(r\"./postmonsoon/ground_water_quality_2018_post.csv\")\n",
    "dataset_df7 = pd.read_csv(r\"./postmonsoon/ground_water_quality_2019_post.csv\")\n",
    "dataset_df8 = pd.read_csv(r\"./postmonsoon/ground_water_quality_2020_post.csv\")\n",
    "dataset_df9 = pd.read_csv(r\"./postmonsoon/ground_water_quality_2021_post.csv\")\n",
    "dataset_df = pd.concat([dataset_df1,dataset_df2,dataset_df3,dataset_df4,dataset_df5,dataset_df6,dataset_df7,dataset_df8,dataset_df9],join='outer',ignore_index=True)\n",
    "dataset_df.drop(columns=['RL_GIS','sno','village','mandal','district','gwl'],inplace=True)\n",
    "dataset_df['season']=dataset_df['season'].astype(str).apply(lambda x: 0 if 'pre' in x.lower() else 1)\n",
    "# End load data\n",
    "\n",
    "# Changing fields to numeric equivalents.\n",
    "dataset_df['pH'] = pd.to_numeric(dataset_df['pH'], errors='coerce')\n",
    "# Now convert to float\n",
    "dataset_df['pH'] = dataset_df['pH'].astype(float)\n",
    "dataset_df['RSC  meq  / L'] = pd.to_numeric(dataset_df['RSC  meq  / L'], errors='coerce')\n",
    "# Now convert to float\n",
    "dataset_df['RSC  meq  / L'] = dataset_df['RSC  meq  / L'].astype(float)\n",
    "# End changing fields\n",
    "\n",
    "# Remove Null values\n",
    "for index, row in dataset_df.iterrows():\n",
    "    if 'NA' in row.values:\n",
    "        dataset_df.drop(index, inplace=True)\n",
    "    elif(row['Classification']=='OG')or(row['Classification']=='O.G')or(row['Classification']=='BELOW THE GRAPH')or(row['Classification']=='OUT OF SAR GRAPH')or(row['Classification']=='BG'):\n",
    "        dataset_df.drop(index, inplace=True)\n",
    "dataset_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "null_threshold = len(dataset_df) * 0.25\n",
    "for column in dataset_df.columns:\n",
    "    null_count = dataset_df[column].isnull().sum()\n",
    "    if null_count > null_threshold:\n",
    "        dataset_df.drop(column, axis=1, inplace=True)\n",
    "    elif null_count > 0:\n",
    "        dataset_df.dropna(subset=[column], inplace=True)\n",
    "dataset_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data = dataset_df\n",
    "# End remove null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K Nearest Neighbours) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K nearest neighbours algorithm is a non-parametric supervised learning algorithm.  \n",
    "Its **training** phase only consists of storing the data.  \n",
    "Its **classification** phase involves of calculating the K nearest neighbours (according to some \"distance\" metric) to the new data for which classification is sought and then finding representative label of those K neighbours and assigning the new data this label. The K mentioned here is given as a parameter and is not learnt, though there are thumb rules (such as sqrt(n), n = # datapoints) and some procedures which can make better choices for K.  \n",
    "\n",
    "<img src=\"kNN_Explanation.png\"/>\n",
    "\n",
    "#### Notes regarding implementation:  \n",
    "There are other kinds of representation of data which help in finding nearest neighbour more quickly than the naive method, which we do not consider here for implementation.  \n",
    "We implement the naive method, which implies that distance of new point from all points is first calculated,   \n",
    "and we then determine the K neighbours, closest to this point, and then simply find the label which occurs maximum number of times  \n",
    "(we have some other rules such as requiring a majority vote, which is defined to be a situation where the majority has to exceed  \n",
    " all others by the proportion of number of classes).   \n",
    "The K chosen in our implementation is sqrt(n), where n is the number of datapoints. Also the metric used is the usual Euclidean Norm/ L2 Norm.    \n",
    "Also, due to non-parallelized implementation, the function is extremely slow and might take around 5-15 minutes to give the results.  \n",
    "The accuracy obtained, however is similar/comparable with the standard library packages implementation. (See below)\n",
    "\n",
    "<img src=\"kNN_Algorithm_Pseudocode.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Implementation without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean metric/L2 norm metric\n",
    "def calculate_distance(row : pd.Series,trow : pd.Series) -> float:\n",
    "    dist = 0\n",
    "    for index,value in row.items():\n",
    "        if index != 'Classification' and index != 'Classification.1':\n",
    "            dist += (value-trow[index])**2\n",
    "    return math.sqrt(dist)\n",
    "\n",
    "# Return the label with maximum count only (No '> 1/unique_label_count' done).\n",
    "def getMajorityLabel(neighbourList: list) -> str:\n",
    "    freqList = []\n",
    "    for item in neighbourList:\n",
    "        for tup in freqList:\n",
    "            if tup[0] == item[1]:\n",
    "                tup[1] = tup[1] + 1\n",
    "                break\n",
    "        else:\n",
    "            freqList.append([item[1],1])\n",
    "    majority_label = ''\n",
    "    max_freq = 0\n",
    "    for item in freqList:\n",
    "        if max_freq < item[1]:\n",
    "            majority_label = item[0]\n",
    "            max_freq = item[1]\n",
    "    return majority_label\n",
    "\n",
    "# Naive KNN\n",
    "# For classification, measure distance of given (new) data to all other test data \n",
    "# and sort the distances and take the least K from these with their labels and\n",
    "# find the majority label among these and assign that label to the given data.\n",
    "def KNN_classifier(dataframe:pd.DataFrame,test_data:pd.DataFrame,K:int) -> None:\n",
    "    req_data = dataframe \n",
    "    req_test_data = test_data\n",
    "    totalTests = req_test_data.shape[0]\n",
    "    totalCorrectClassifications = 0\n",
    "    for tindex,trow in req_test_data.iterrows():\n",
    "        nearest_neighbours = []\n",
    "        for index,row in req_data.iterrows():\n",
    "            nearest_neighbours.append((calculate_distance(row,trow),row['Classification']))\n",
    "        nearest_neighbours.sort()\n",
    "        nearest_neighbours = nearest_neighbours[0:K-1]\n",
    "        if trow['Classification'] == getMajorityLabel(nearest_neighbours):\n",
    "            totalCorrectClassifications = totalCorrectClassifications+1\n",
    "    print(totalCorrectClassifications/totalTests *100)\n",
    "\n",
    "# Split data into 80-20 as train and test data sets\n",
    "train = data.sample(frac=0.8,random_state=8)\n",
    "test = data.drop(train.index)\n",
    "\n",
    "# Parameter K : first choice\n",
    "K =  math.ceil(math.sqrt(train.shape[0]))\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the KNN on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.39213602550478\n"
     ]
    }
   ],
   "source": [
    "# Actual Classification phase.\n",
    "KNN_classifier(train,test,K=K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn KNN to compare performance with above implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.49840595111584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\p0803\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# sklearn requires numerical labels for output also, so use label encoding for categories\n",
    "label_encoder = LabelEncoder()\n",
    "data['encoded_Classification'] = label_encoder.fit_transform(data['Classification'])\n",
    "\n",
    "# Same split\n",
    "train = data.sample(frac=0.8,random_state=8)\n",
    "test = data.drop(train.index)\n",
    "\n",
    "# sklearn requires separate input data and output labels (encoded)\n",
    "trainX = train.drop(['Classification','encoded_Classification','Classification.1'],axis=1)\n",
    "trainY = train['encoded_Classification']\n",
    "\n",
    "testX = test.drop(['Classification','encoded_Classification','Classification.1'],axis=1)\n",
    "testY = test['encoded_Classification']\n",
    "\n",
    "# Parameter K : first choice\n",
    "K =  math.ceil(math.sqrt(train.shape[0]))\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=K)\n",
    "knn_classifier.fit(trainX,trainY)\n",
    "\n",
    "# Inbuilt function to calculate accuracy on test data set\n",
    "accuracy = knn_classifier.score(testX, testY)\n",
    "print(100*accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
